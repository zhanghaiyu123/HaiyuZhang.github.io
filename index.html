<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset="UTF-8"";> 
		<title>Haiyu Zhang's Homepage</title> 
		<style>
			body{
				margin-top: 20px; 
				margin-left: 100px;
				margin-right: 100px; 
			    }
		</style>
	</head>

	<body>
		
	<h2>Haiyu Zhang's homepage</h2>

	I am a second-year master student in mathematics at Southern University of Science and Technology (SUSTech) in ShenZhen, China. My advisor is <a href="https://yifeizhu.github.io/" target="_blank">Yifei Zhu</a>. I received my B.S. from SUSTech in 2024.<br> <br> 
	
	My research interests lie in the mathematical foundations of machine learning, particularly in understanding the generalization behavior and interpretability of neural networks. I am also interested in designing and improving theoretically grounded machine learning algorithms and models. My previous work primarily focused on deep learning theory and the intersection of machine learning with geometry and topology.<br> <br> 
	Email:  <a href="mailto:12432036@sustech.edu.cn"> 12432036@sustech.edu.cn </a> <br> <br> 
	You can find my CV <a href="Research/cv__Haiyu.pdf" target="_blank">here</a>.



<h3>Papers</h3>
   <p>
  <a href="https://yifeizhu.github.io/tail.pdf" target="_blank">Topology-enhanced machine learning for consonant recognition</a>, 
	 Pingyao Feng, Qingrui Qu, Haiyu Zhang, Siheng Yi, Zhiwang Yu, Zeyang Ding and Yifei Zhu, 
  Preprint.  
</p>

<!-- <p> 
  <a href="https://yifeizhu.github.io/topker.pdf" target="_blank">Topological deep learning for speech recognition</a>, 
	Zhiwang Yu, Pingyao Feng, Qingrui Qu, Haiyu Zhang and Yifei Zhu,
  Preprint.  
</p> -->


<h3>Presentations</h3>
			<!-- <li>  -->
		<div class="talk-entry">
			<details>
				<summary>
					<a href="Research/Intrinsic_dimension_slides.pdf" target="_blank">Fractal Structure and Generalization Bounds with Intrinsic Dimension for Stochastic Optimization Algorithms</a> 
					<br> <br>
				</summary>
				The concept of "intrinsic dimension" (ID) is motivated by Leo Breiman's first question: Why don't heavily parameterized neural networks overfit? Existing studies suggest that the fractal dimension of weight trajectories, which is regarded as ID, can serve as an intrinsic measure of "effective" hypothesis complexity to shed light on the generalization mystery. A rigorous framework for generalization bounds, established through PAC-Bayesian theory, has demonstrated a positive correlation between ID and the generalization gap.
This talk reviews these existing results, elaborates on the limitations of current research, and proposes a research plan based on these findings.
			</details>
			</div>
		<!-- </li> -->

			<br> <br>

					<!-- <li>  -->
		<div class="talk-entry">
			<details>
				<summary>
					<a href="Research/Uderstanding_neuralnetworks_slides.pdf" target="_blank">Understanding Neural Networks: A Perspective on Representability and Interpretability</a> 
					<br> <br>
				</summary>
			 This talk reviews the work on neural network approximation theory, explaining the roles of depth and width in function approximation and representability of different architectures(MLP, RNN, Transformer, etc.). Furthermore, it presents some results of visualizing the training process of neural networks from geometric and topological perspectives, and discusses the grokking phenomenon.
			</details>
			</div>
		<!-- </li> -->

			<br> <br>

							<!-- <li>  -->
					<div class="talk-entry">			
			<details>
				<summary>
					<a href="Research/TDA_TDL_slides.pdf" target="_blank">Topological Data Analysis and Topological Deep Learning: with Applications to Image Data</a>
						<br> <br>
				</summary>
			 This talk summarizes the foundational work by Carlsson et al. on Topological Data Analysis and Topological Deep Learning in image processing, and presents some results of reproduction and improvement of relevant models.

It begins by reviewing the groundbreaking TDA study on natural image patches in <a href="https://link.springer.com/article/10.1007/s11263-007-0056-x" target="_blank">Carlsson et al. 2008</a>, which extracted a submanifold with the topology of the Klein bottle. The talk then presents results reproducing the work of  <a href="https://arxiv.org/abs/1810.03234" target="_blank">Gabrielsson and Carlsson 2019</a> on the topology of neural networks using Persistent Homology. These findings show that CNN kernels progressively learn topological structures (circles, Klein bottles) that align with the intrinsic topology of high-contrast image patches. These findings
 suggest that learned convolution kernels act as discrete first- and second-order differential operators(e.g., Prewitt, Laplacian), enabling the extraction of local features such as edges and textures by taking the inner product with local patches of the image. 
Building on this insight, and inspired by <a href="https://jmlr.org/papers/v24/21-0073.html" target="_blank">Love et al. 2023</a>, an optimized topological CNN model for image classification is proposed. This model improves performance by embedding the topological structure into the neural network and sampling convolution kernels from non-uniform probability distributions defined on these manifolds.
			</details>
				</div>		
		<!-- </li> -->
			<br> <br>

					<!-- <li>  --> <div class="talk-entry">
			<details>
				<summary>
					<a href="Research/Entropy_slides.pdf" target="_blank">Entropy of Measure Preserving Endomorphisms</a> 
					<br> <br>
				</summary>
		This talk primarily references the first chapter of the book, <a href="https://www.cambridge.org/core/books/conformal-fractals/2D571156235E48946568D59B1FFDAA0E" target="_blank">Conformal Fractals â€“ Ergodic Theory Methods</a>. It introduces the motivations, definitions and properties of the conditional entropy of partitions and the measure-theoretic entropy of measure preserving endomorphisms.
			</details> 
						</div>
		<!-- </li> -->
			

		<h3>Conferences and workshops</h3>
		 <p>
  <a href="https://iwoat.github.io/2025/special" target="_blank">International Workshop on Algebraic Topology (IWoAT)</a>, SUSTech, 2025. 
</p>
		
			 <p>
  <a href="https://www.maths.ox.ac.uk/groups/ml-and-ds/topological-data-analysis/spires-2024" target="_blank"> 4th annual Centre for Topological Data Analysis conference (Spires 2024)</a>, University of Oxford, 2024. A  <a href="https://yifeizhu.github.io/tail-poster.pdf" target="_blank">poster</a> was presented at poster session.
</p>

					 <p>
  <a href="https://gbatc.github.io/" target="_blank">Greater Bay Area Topology Conference</a>, SUSTech, 2024.
</p>


		<h3>Teaching</h3>
		 <p>Teaching assistant for <a href="https://yifeizhu.github.io/341/" target="_blank">Applied and Computational Topology</a>, 2025 Fall <p>
		 <p>Teaching assistant for Calculus, 2024 Fall <p>
		
		

		       	   
		 </ul>
		</div>


	</body>
	</html>
